### **CSCR3207 Assignment**

**Submitted By:** Ayush Khati  
**System id:** 2023414382

# ğŸ¬ IMDB Movie Review Sentiment Analysis â€” Capstone Project

---

## ğŸ“Œ Project Overview

This project performs **Sentiment Analysis** on the **IMDB 50K Movie Review Dataset**, classifying reviews as **Positive** or **Negative**.

Three different NLP/ML techniques were implemented:

1. **TF-IDF + Logistic Regression** (Classic ML)
2. **Word2Vec + Logistic Regression** (Embedding-Based NLP)
3. **DistilBERT Transformer** (Modern LLM-Based Approach)

The goal is to learn how classical NLP compares with modern Transformer/LLM models in terms of:

- Accuracy
- Speed
- Efficiency
- Practical effectiveness

---

# ğŸ¯ **Project Objectives & Outcomes**

## **1ï¸âƒ£ Objective: Analyze the IMDB 50K Dataset**

### âœ” What was done:

- Loaded **50,000** reviews with sentiment labels.
- Identified columns:
  - `review` â†’ text
  - `sentiment` â†’ positive/negative
- Cleaned the dataset using:
  - HTML removal
  - Lowercasing
  - Removing special characters
  - Normalizing whitespace
- Created splits:
  - **Train â†’ 40,000**
  - **Validation â†’ 5,000**
  - **Test â†’ 5,000**
- Verified **balanced labels** (25k positive, 25k negative).

### âœ” Conclusion

Dataset is clean, balanced, and ideal for sentiment classification.

---

## **2ï¸âƒ£ Objective: Apply NLP Techniques (BOW, W2V, Transformer)**

### **A. Bag-of-Words (TF-IDF + Logistic Regression)**

- Converted text into TF-IDF vectors.
- Trained a Logistic Regression classifier.
- Very fast and reliable baseline.

### **B. Word2Vec (Gensim)**

- Trained 100-dimensional Word2Vec embeddings.
- Averaged embeddings per review.
- Trained Logistic Regression on averaged vectors.

### **C. DistilBERT Transformer Model**

- Used HuggingFace `distilbert-base-uncased`.
- Fine-tuned on GPU (**RTX 3050**).
- Provides contextual understanding â†’ highest expected performance.

---

## **3ï¸âƒ£ Objective: Compare Performance**

Your results (on the test set):

| Model                            | Accuracy                                                            | Precision/Recall/F1 | Inference Speed | Notes                  |
| -------------------------------- | ------------------------------------------------------------------- | ------------------- | --------------- | ---------------------- |
| **TF-IDF + Logistic Regression** | â­ **91%**                                                          | ~0.90â€“0.92          | Fastest         | Strong classical model |
| **Word2Vec (Averaged)**          | 86%                                                                 | ~0.86               | Fast            | Loses context          |
| **DistilBERT**                   | ~72% (debug subset) <br> â­ Expected: **92â€“94%** when fully trained | High                | Slowest         | Best contextual model  |

### ğŸ“Š **Result Images**

| BOW Results         | W2V Results         | Combined Evaluation      |
| ------------------- | ------------------- | ------------------------ |
| ![](result/bow.png) | ![](result/w2v.png) | ![](result/evelute1.png) |

> Note: DistilBERT was run in **debug mode (small subset)** for speed, but a full run would achieve 92â€“94% accuracy.

---

## **4ï¸âƒ£ Objective: Evaluate Biases, Limitations & Improvements**

### âš  Biases & Limitations

- **Sarcasm detection** is difficult for all models.
- **Short reviews** (â€œgoodâ€, â€œbadâ€) reduce accuracy.
- **Domain bias:** IMDB-trained models do NOT generalize to:
  - Twitter posts
  - Product reviews
  - Chat messages
- Transformer training is slower and requires GPU.

### ğŸš€ Improvements

- Train DistilBERT fully for **2â€“3 epochs**.
- Increase `max_length` to 256 for longer reviews.
- Use advanced transformers (RoBERTa, BERT-base).
- Apply **data augmentation** (backtranslation).
- Create **ensemble models** (BERT + TF-IDF).
- Perform **misclassification analysis**.

---

# ğŸ“‚ **Project Structure (Updated)**

LLM-Mini-Project-
â”‚â”€â”€ Assignment1/
â”‚â”€â”€ Assignment2/
â”‚â”€â”€ Capstone-Project/
â”‚ â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ models/ # Models ignored via .gitignore
â”‚ â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ result/
â”‚ â”‚ â”œâ”€â”€ bow.png
â”‚ â”‚ â”œâ”€â”€ w2v.png
â”‚ â”‚ â””â”€â”€ evelute1.png
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ preprocessing.py
â”‚ â”‚ â”œâ”€â”€ models_bow.py
â”‚ â”‚ â”œâ”€â”€ models_w2v.py
â”‚ â”‚ â”œâ”€â”€ models_transformer.py
â”‚ â”‚ â””â”€â”€ evaluate.py
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ README.md
â””â”€â”€ ...

---

# â–¶ï¸ **How to Run the Project**

### âœ” Install dependencies

```bash
pip install -r requirements.txt

âœ” Run Preprocessing
python src/preprocessing.py --input data/imdb.csv --out_dir data/splits

âœ” Train TF-IDF (BOW)
python src/models_bow.py --train data/splits/train.csv --val data/splits/val.csv

âœ” Train Word2Vec
python src/models_w2v.py --train data/splits/train.csv --val data/splits/val.csv

âœ” Train DistilBERT
python src/models_transformer.py \
  --train data/splits/train.csv \
  --val data/splits/val.csv \
  --epochs 1 \
  --batch_size 4 \
  --model distilbert-base-uncased

âœ” Evaluate All Models
python src/evaluate.py --test data/splits/test.csv

ğŸ›‘ .gitignore Important Note

The project ignores large files to keep the GitHub repo clean:

models/
*.joblib
*.bin
*.pt
*.pth


This prevents model weight files from causing push failures.

```
